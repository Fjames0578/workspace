import numpy as np

# Define the ReLU activation function and its derivative
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# Define the mean squared error loss function and its derivative
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def mse_derivative(y_true, y_pred):
    return 2 * (y_pred - y_true) / y_true.size

# Initialize weights and biases
input_size = 10
hidden_size = 5
output_size = 1

np.random.seed(0)
weights1 = np.random.randn(input_size, hidden_size)
biases1 = np.zeros(hidden_size)
weights2 = np.random.randn(hidden_size, output_size)
biases2 = np.zeros(output_size)

# Define the learning rate and number of epochs
learning_rate = 0.01
epochs = 1000

# Training loop
for epoch in range(epochs):
    # Forward pass
    hidden_layer = relu(np.dot(input_data, weights1) + biases1)
    output_layer = np.dot(hidden_layer, weights2) + biases2

    # Compute the loss
    loss = mse(target_data, output_layer)

    # Backward pass
    output_error_signal = mse_derivative(target_data, output_layer)
    hidden_error_signal = np.dot(output_error_signal, weights2.T) * relu_derivative(hidden_layer)

    # Update weights and biases
    weights2 -= learning_rate * np.dot(hidden_layer.T, output_error_signal)
    biases2 -= learning_rate * np.sum(output_error_signal, axis=0)
    weights1 -= learning_rate * np.dot(input_data.T, hidden_error_signal)
    biases1 -= learning_rate * np.sum(hidden_error_signal, axis=0)

    # Print the loss every 100 epochs
    if epoch % 100 == 0:
        print('Epoch %d, Loss %.3f' % (epoch, loss))